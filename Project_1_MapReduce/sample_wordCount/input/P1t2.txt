Despite the success of kernelization, the basic definition has an important drawback it does not combine well with approximation algorithms or with heuristics. This is a serious problem since after all the ultimate goal of parameterized algorithms, or for that matter of any algorithmic paradigm, is to eventually solve the given input instance. Thus, the application of a preprocessing algorithm is always followed by an algorithm that finds a solution to the reduced instance. In practice, even after applying a preprocessing procedure, the reduced instance may not be small enough to be solved to optimality within a reasonable time bound. In these cases one gives up on optimality and resorts to approximation algorithms or heuristics instead. Thus it is crucial that the solution obtained by an approximation algorithm or heuristic when run on the reduced instance provides a good solution to the original instance, or at least some meaningful information about the original instance. The current definition of kernels allows for kernelization algorithms with the unsavory property that running an approximation algorithm or heuristic on the reduced instance provides no insight whatsoever about the original instance.
There is a lack of, and a real need for, a mathematical framework for analysing the performance of preprocessing algorithms, such that the framework not only combines well with parameterized and exact exponential time algorithms, but also with approximation algorithms and heuristics. Our main conceptual contribution is an attempt at building such a framework.
The main reason that the existing notion of kernelization does not combine well with approximation algorithms is that the definition of a kernel is deeply rooted in decision problems. The starting point of our new framework is an extension of kernelization to optimization problems.
To demonstrate versatility we show that our framework can be deployed to measure the efficiency of preprocessing heuristics both in terms of the value of the optimum solution, and in terms of structural properties of the input instance that do not necessarily have any relation to the value of the optimum. In the language of parameterized complexity, we show that framework captures approximate kernels both for problems parameterized by the value of the optimum, and for structural parameterizations.  
A common feature of the above examples of approximate kernels is that they beat both the known lower bounds on kernel size of traditional kernels and the lower bounds on approximation ratios of approximation algorithms.  
If this is the case, it would offer up at least a partial explanation of why preprocessing heuristics combined with brute force search perform so much better than what is predicted by hardness of approximation results and kernelization lower bounds. This gives another compelling reason for a systematic investigation of lossy kernelization of parameterized optimization problems.
The observation that a lossy preprocessing can simultaneously achieve a better size bound than normal kernelization algorithms as well as a better approximation factor than the ratio of the best approximation algorithms is not new.  
Unfortunately this definition suffers from the same serious drawback as the original definition of kernels it does not combine well with approximation algorithms or with heuristics. Indeed, in the context of lossy preprocessing this drawback is even more damning, as there is no reason why one should allow a loss of precision in the preprocessing step, but demand that the reduced instance has to be solved to optimality.
